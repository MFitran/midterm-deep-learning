{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Clustering Assignment: Credit Card Customer Segmentation**\n",
                "\n",
                "### 1. Project Overview and Problem Statement\n",
                "\n",
                "This notebook implements a **Clustering** solution for customer segmentation using the provided credit card dataset. The primary goal is to discover distinct groups of customers based on their spending and payment behavior (unsupervised learning).\n",
                "\n",
                "The insights derived from these clusters can be used by the bank for targeted marketing, customized product offerings, and risk management.\n",
                "\n",
                "| Component | Description |\n",
                "| :--- | :--- |\n",
                "| **Task** | Unsupervised Learning: Clustering (Customer Segmentation) |\n",
                "| **Dataset** | Credit Card Dataset (`clusteringmidterm.csv`) with features like `BALANCE`, `PURCHASES`, `CREDIT_LIMIT`, etc. |\n",
                "| **Algorithm** | $\\mathbf{K}$-Means Clustering. |\n",
                "| **Evaluation** | Elbow Method for $\\mathbf{K}$ determination, and Cluster Profiling (analyzing mean feature values). |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Data Loading and Initial Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.metrics import silhouette_score\n",
                "\n",
                "# Load the dataset\n",
                "file_name = 'clusteringmidterm.csv' \n",
                "try:\n",
                "    df = pd.read_csv(file_name)\n",
                "    print(\"Data loaded successfully.\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"Error: File '{file_name}' not found.\")\n",
                "    # Stop execution if file is not found (or load a dummy, but we expect the real file here)\n",
                "    raise\n",
                "\n",
                "# Display the first 5 rows and data information\n",
                "print(\"\\n--- Data Head ---\")\n",
                "print(df.head())\n",
                "\n",
                "print(\"\\n--- Data Info ---\")\n",
                "print(df.info())\n",
                "\n",
                "print(\"\\n--- Missing Values Check ---\")\n",
                "print(df.isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Data Preprocessing\n",
                "\n",
                "Clustering algorithms, especially $\\mathbf{K}$-Means, are highly sensitive to unscaled data and missing values. We perform imputation and feature scaling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1 Handle Missing Values\n",
                "# Impute missing values in 'MINIMUM_PAYMENTS' and 'CREDIT_LIMIT' with the median.\n",
                "imputer = SimpleImputer(strategy='median')\n",
                "\n",
                "# We only impute the specific columns\n",
                "df[['MINIMUM_PAYMENTS', 'CREDIT_LIMIT']] = imputer.fit_transform(df[['MINIMUM_PAYMENTS', 'CREDIT_LIMIT']])\n",
                "\n",
                "# Verify imputation\n",
                "print(\"\\n--- Missing Values Check After Imputation ---\")\n",
                "print(df[['MINIMUM_PAYMENTS', 'CREDIT_LIMIT']].isnull().sum())\n",
                "\n",
                "# 3.2 Prepare Data for Clustering\n",
                "# Drop the non-feature Customer ID\n",
                "X = df.drop('CUST_ID', axis=1)\n",
                "\n",
                "# 3.3 Feature Scaling\n",
                "# Scale all features to ensure equal contribution to distance calculation\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
                "\n",
                "print(\"\\n--- Scaled Data Head ---\")\n",
                "print(X_scaled_df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Optimal Cluster Determination ($\\mathbf{K}$)\n",
                "\n",
                "We use the **Elbow Method** to estimate the optimal number of clusters ($\\mathbf{K}$) by looking for the 'bend' in the WCSS (Within-Cluster Sum of Squares) plot. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate WCSS for a range of K values\n",
                "wcss = []\n",
                "K_range = range(1, 11)\n",
                "\n",
                "for k in K_range:\n",
                "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
                "    kmeans.fit(X_scaled_df)\n",
                "    wcss.append(kmeans.inertia_)\n",
                "\n",
                "# Plot the Elbow Method\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(K_range, wcss, marker='o', linestyle='--')\n",
                "plt.title('Elbow Method for Optimal K')\n",
                "plt.xlabel('Number of Clusters (K)')\n",
                "plt.ylabel('WCSS')\n",
                "plt.xticks(K_range)\n",
                "plt.grid(True)\n",
                "plt.savefig('elbow_method.png')\n",
                "plt.show()\n",
                "\n",
                "# Based on visual inspection, we choose an optimal K\n",
                "optimal_k = 4 # Example based on typical results for this dataset\n",
                "print(f\"Chosen Optimal K based on Elbow Method: {optimal_k}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Model Training (K-Means Clustering)\n",
                "\n",
                "We train the $\\mathbf{K}$-Means model using the chosen $\\mathbf{K}$ and assign the cluster labels back to the original dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train K-Means with the determined optimal K\n",
                "final_k = optimal_k # Use the K determined in the previous step\n",
                "kmeans_final = KMeans(n_clusters=final_k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
                "clusters = kmeans_final.fit_predict(X_scaled_df)\n",
                "\n",
                "# Add cluster labels to the original (unscaled) DataFrame\n",
                "df['Cluster'] = clusters\n",
                "\n",
                "# Calculate Silhouette Score (optional, but good for validation)\n",
                "silhouette_avg = silhouette_score(X_scaled_df, clusters)\n",
                "\n",
                "print(f\"Clustering complete. {final_k} clusters found.\")\n",
                "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
                "print(\"\\nData head with cluster labels:\")\n",
                "print(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Evaluation and Interpretation\n",
                "\n",
                "#### 6.1 Dimensionality Reduction for Visualization (PCA)\n",
                "\n",
                "We use **Principal Component Analysis (PCA)** to reduce the 17 dimensions down to 2, allowing us to visualize the clusters effectively. 

[Image of PCA decomposition]
"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply PCA to reduce dimensionality to 2 components\n",
                "pca = PCA(n_components=2)\n",
                "principal_components = pca.fit_transform(X_scaled_df)\n",
                "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
                "\n",
                "# Add cluster labels for plotting\n",
                "pca_df['Cluster'] = df['Cluster']\n",
                "\n",
                "# Visualize the clusters\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.scatterplot(\n",
                "    x='PC1', y='PC2', hue='Cluster', data=pca_df,\n",
                "    palette=sns.color_palette('tab10', final_k), legend='full', alpha=0.7\n",
                ")\n",
                "plt.title(f'Clusters visualized using PCA ({final_k} Clusters)')\n",
                "plt.xlabel('Principal Component 1')\n",
                "plt.ylabel('Principal Component 2')\n",
                "plt.legend(title='Cluster')\n",
                "plt.savefig('pca_cluster_visualization.png')\n",
                "plt.show()\n",
                "\n",
                "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
                "print(f\"Total variance explained by 2 components: {pca.explained_variance_ratio_.sum():.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### 6.2 Cluster Profiling\n",
                "\n",
                "Analyze the mean values of key features for each cluster to understand their characteristics.\n",
                "\n",
                "# Select key columns for profiling (using original unscaled data)\n",
                "profiling_cols = ['Cluster', 'BALANCE', 'PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'PRC_FULL_PAYMENT']\n",
                "cluster_profile = df[profiling_cols].groupby('Cluster').mean()\n",
                "\n",
                "print(\"\\n--- Cluster Profiles (Mean Feature Values) ---\")\n",
                "print(cluster_profile.T)\n",
                "\n",
                "# Conclusion based on profiles\n",
                "print(\"\\n--- Conclusion ---\")\n",
                "print(\"By analyzing the cluster profiles, we can characterize each customer segment:\")\n",
                "print(\"Example Interpretation (to be verified based on your final data):\")\n",
                "print(\"- Cluster 0: High Balance, High Purchases, High Payments (Affluent Users)\")\n",
                "print(\"- Cluster 1: High Cash Advance, Low Purchases (Cash-focused Users)\")\n",
                "print(\"- Cluster 2: Low Everything (Inconsistent/Small Spenders)\")\n",
                "print(\"- Cluster 3: High Credit Limit, High PRC_FULL_PAYMENT (Responsible, High-Limit Users)\")\n",
                "print(\"\\nThese profiles guide business strategy by defining distinct customer segments.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
